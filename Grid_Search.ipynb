{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba-Jo_KnZdCg",
        "outputId": "3b3d1b7f-eaf7-4fa6-dd66-3247d5104a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from statistics import mean\n",
        "import random\n",
        "from pandas import DataFrame\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "from timeit import default_timer as timer\n",
        "matplotlib.use('Agg')\n",
        "from matplotlib import pyplot\n",
        "%matplotlib inline\n",
        "\n",
        "top_words = 5000\n",
        "(x_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dimension = 50\n",
        "top_words = 5000\n",
        "\n",
        "def get_data(maxlen):\n",
        "    x = sequence.pad_sequences(x_train, maxlen=maxlen,padding='post',truncating='post')\n",
        "    return x,y_train"
      ],
      "metadata": {
        "id": "l1MmkP6JZdjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "learning_rate = [0.006,0.004,0.005]\n",
        "n_neurons = [160,416,180]\n",
        "n_timesteps = [340,340,296]\n",
        "n_epochs = [14,13,2]\n",
        "\n",
        "\n",
        "hyper_para = (learning_rate, n_neurons, n_timesteps, n_epochs)\n",
        "hyper_para_grid = tuple(itertools.product(*hyper_para, repeat=1))\n",
        "print(len(hyper_para_grid))\n",
        "for learning_rate, n_neurons, n_timesteps, n_epochs in hyper_para_grid:\n",
        "  print('lr',learning_rate, 'n_neurons', n_neurons, 'n_timesteps', n_timesteps,'n_epochs', n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaP3LciBZpBw",
        "outputId": "8be34489-0d3c-483a-b86a-67b7cf639992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81\n",
            "lr 0.006 n_neurons 160 n_timesteps 340 n_epochs 14\n",
            "lr 0.006 n_neurons 160 n_timesteps 340 n_epochs 13\n",
            "lr 0.006 n_neurons 160 n_timesteps 340 n_epochs 2\n",
            "lr 0.006 n_neurons 160 n_timesteps 340 n_epochs 14\n",
            "lr 0.006 n_neurons 160 n_timesteps 340 n_epochs 13\n",
            "lr 0.006 n_neurons 160 n_timesteps 340 n_epochs 2\n",
            "lr 0.006 n_neurons 160 n_timesteps 296 n_epochs 14\n",
            "lr 0.006 n_neurons 160 n_timesteps 296 n_epochs 13\n",
            "lr 0.006 n_neurons 160 n_timesteps 296 n_epochs 2\n",
            "lr 0.006 n_neurons 416 n_timesteps 340 n_epochs 14\n",
            "lr 0.006 n_neurons 416 n_timesteps 340 n_epochs 13\n",
            "lr 0.006 n_neurons 416 n_timesteps 340 n_epochs 2\n",
            "lr 0.006 n_neurons 416 n_timesteps 340 n_epochs 14\n",
            "lr 0.006 n_neurons 416 n_timesteps 340 n_epochs 13\n",
            "lr 0.006 n_neurons 416 n_timesteps 340 n_epochs 2\n",
            "lr 0.006 n_neurons 416 n_timesteps 296 n_epochs 14\n",
            "lr 0.006 n_neurons 416 n_timesteps 296 n_epochs 13\n",
            "lr 0.006 n_neurons 416 n_timesteps 296 n_epochs 2\n",
            "lr 0.006 n_neurons 180 n_timesteps 340 n_epochs 14\n",
            "lr 0.006 n_neurons 180 n_timesteps 340 n_epochs 13\n",
            "lr 0.006 n_neurons 180 n_timesteps 340 n_epochs 2\n",
            "lr 0.006 n_neurons 180 n_timesteps 340 n_epochs 14\n",
            "lr 0.006 n_neurons 180 n_timesteps 340 n_epochs 13\n",
            "lr 0.006 n_neurons 180 n_timesteps 340 n_epochs 2\n",
            "lr 0.006 n_neurons 180 n_timesteps 296 n_epochs 14\n",
            "lr 0.006 n_neurons 180 n_timesteps 296 n_epochs 13\n",
            "lr 0.006 n_neurons 180 n_timesteps 296 n_epochs 2\n",
            "lr 0.004 n_neurons 160 n_timesteps 340 n_epochs 14\n",
            "lr 0.004 n_neurons 160 n_timesteps 340 n_epochs 13\n",
            "lr 0.004 n_neurons 160 n_timesteps 340 n_epochs 2\n",
            "lr 0.004 n_neurons 160 n_timesteps 340 n_epochs 14\n",
            "lr 0.004 n_neurons 160 n_timesteps 340 n_epochs 13\n",
            "lr 0.004 n_neurons 160 n_timesteps 340 n_epochs 2\n",
            "lr 0.004 n_neurons 160 n_timesteps 296 n_epochs 14\n",
            "lr 0.004 n_neurons 160 n_timesteps 296 n_epochs 13\n",
            "lr 0.004 n_neurons 160 n_timesteps 296 n_epochs 2\n",
            "lr 0.004 n_neurons 416 n_timesteps 340 n_epochs 14\n",
            "lr 0.004 n_neurons 416 n_timesteps 340 n_epochs 13\n",
            "lr 0.004 n_neurons 416 n_timesteps 340 n_epochs 2\n",
            "lr 0.004 n_neurons 416 n_timesteps 340 n_epochs 14\n",
            "lr 0.004 n_neurons 416 n_timesteps 340 n_epochs 13\n",
            "lr 0.004 n_neurons 416 n_timesteps 340 n_epochs 2\n",
            "lr 0.004 n_neurons 416 n_timesteps 296 n_epochs 14\n",
            "lr 0.004 n_neurons 416 n_timesteps 296 n_epochs 13\n",
            "lr 0.004 n_neurons 416 n_timesteps 296 n_epochs 2\n",
            "lr 0.004 n_neurons 180 n_timesteps 340 n_epochs 14\n",
            "lr 0.004 n_neurons 180 n_timesteps 340 n_epochs 13\n",
            "lr 0.004 n_neurons 180 n_timesteps 340 n_epochs 2\n",
            "lr 0.004 n_neurons 180 n_timesteps 340 n_epochs 14\n",
            "lr 0.004 n_neurons 180 n_timesteps 340 n_epochs 13\n",
            "lr 0.004 n_neurons 180 n_timesteps 340 n_epochs 2\n",
            "lr 0.004 n_neurons 180 n_timesteps 296 n_epochs 14\n",
            "lr 0.004 n_neurons 180 n_timesteps 296 n_epochs 13\n",
            "lr 0.004 n_neurons 180 n_timesteps 296 n_epochs 2\n",
            "lr 0.005 n_neurons 160 n_timesteps 340 n_epochs 14\n",
            "lr 0.005 n_neurons 160 n_timesteps 340 n_epochs 13\n",
            "lr 0.005 n_neurons 160 n_timesteps 340 n_epochs 2\n",
            "lr 0.005 n_neurons 160 n_timesteps 340 n_epochs 14\n",
            "lr 0.005 n_neurons 160 n_timesteps 340 n_epochs 13\n",
            "lr 0.005 n_neurons 160 n_timesteps 340 n_epochs 2\n",
            "lr 0.005 n_neurons 160 n_timesteps 296 n_epochs 14\n",
            "lr 0.005 n_neurons 160 n_timesteps 296 n_epochs 13\n",
            "lr 0.005 n_neurons 160 n_timesteps 296 n_epochs 2\n",
            "lr 0.005 n_neurons 416 n_timesteps 340 n_epochs 14\n",
            "lr 0.005 n_neurons 416 n_timesteps 340 n_epochs 13\n",
            "lr 0.005 n_neurons 416 n_timesteps 340 n_epochs 2\n",
            "lr 0.005 n_neurons 416 n_timesteps 340 n_epochs 14\n",
            "lr 0.005 n_neurons 416 n_timesteps 340 n_epochs 13\n",
            "lr 0.005 n_neurons 416 n_timesteps 340 n_epochs 2\n",
            "lr 0.005 n_neurons 416 n_timesteps 296 n_epochs 14\n",
            "lr 0.005 n_neurons 416 n_timesteps 296 n_epochs 13\n",
            "lr 0.005 n_neurons 416 n_timesteps 296 n_epochs 2\n",
            "lr 0.005 n_neurons 180 n_timesteps 340 n_epochs 14\n",
            "lr 0.005 n_neurons 180 n_timesteps 340 n_epochs 13\n",
            "lr 0.005 n_neurons 180 n_timesteps 340 n_epochs 2\n",
            "lr 0.005 n_neurons 180 n_timesteps 340 n_epochs 14\n",
            "lr 0.005 n_neurons 180 n_timesteps 340 n_epochs 13\n",
            "lr 0.005 n_neurons 180 n_timesteps 340 n_epochs 2\n",
            "lr 0.005 n_neurons 180 n_timesteps 296 n_epochs 14\n",
            "lr 0.005 n_neurons 180 n_timesteps 296 n_epochs 13\n",
            "lr 0.005 n_neurons 180 n_timesteps 296 n_epochs 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_grid, val_loss_grid = list(), list()\n",
        "best_params = []\n",
        "def fit_lstm_grid(batch_size):\n",
        "  min_val_loss = 999\n",
        "  current_val_loss = 0\n",
        "  for learning_rate, n_neurons, n_timesteps, n_epochs in hyper_para_grid:\n",
        "    print('lr',learning_rate, 'n_neurons', n_neurons, 'n_timesteps', n_timesteps,'n_epochs', n_epochs)\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(top_words, embedding_dimension,input_length=n_timesteps,mask_zero=True))\n",
        "    model.add(LSTM(n_neurons,return_sequences=False))\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',optimizer=Adam(lr=learning_rate),metrics=['accuracy'])\n",
        "    # fit model\n",
        "    get_train_x,label_array = get_data(n_timesteps)\n",
        "    lstm_model = model.fit(get_train_x,label_array,epochs=n_epochs,batch_size=batch_size, validation_split=0.2)\n",
        "    current_val_loss = lstm_model.history['val_loss'][-1]\n",
        "    if current_val_loss < min_val_loss:\n",
        "      min_val_loss = current_val_loss\n",
        "      best_params = [learning_rate,n_neurons,n_timesteps,n_epochs]\n",
        "      print('best_params',best_params)\n",
        "    \n",
        "  return best_params"
      ],
      "metadata": {
        "id": "qg-WNVUGaQOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_lstm_grid():\n",
        "  n_batch = 100\n",
        "  best_params = fit_lstm_grid(n_batch)\n",
        "  return best_params"
      ],
      "metadata": {
        "id": "vdT0cvpKaVJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = timer()\n",
        "best_params = run_lstm_grid()\n",
        "end=timer()\n",
        "print(\"Time Taken -> \",str(end-start))\n",
        "print(\"Best params \",best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw-49makaXcX",
        "outputId": "e9f64011-655c-46ac-ba9f-7ffe7320d393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr 0.006 n_neurons 160 n_timesteps 340 n_epochs 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/14\n",
            "200/200 [==============================] - 331s 2s/step - loss: 0.5728 - accuracy: 0.6747 - val_loss: 0.3764 - val_accuracy: 0.8402\n",
            "Epoch 2/14\n",
            "200/200 [==============================] - 315s 2s/step - loss: 0.3293 - accuracy: 0.8653 - val_loss: 0.3426 - val_accuracy: 0.8582\n",
            "Epoch 3/14\n",
            "200/200 [==============================] - 332s 2s/step - loss: 0.2706 - accuracy: 0.8962 - val_loss: 0.3202 - val_accuracy: 0.8654\n",
            "Epoch 4/14\n",
            "200/200 [==============================] - 331s 2s/step - loss: 0.2424 - accuracy: 0.9082 - val_loss: 0.3686 - val_accuracy: 0.8656\n",
            "Epoch 5/14\n",
            "200/200 [==============================] - 329s 2s/step - loss: 0.2118 - accuracy: 0.9211 - val_loss: 0.3699 - val_accuracy: 0.8600\n",
            "Epoch 6/14\n",
            "200/200 [==============================] - 329s 2s/step - loss: 0.1897 - accuracy: 0.9303 - val_loss: 0.3513 - val_accuracy: 0.8662\n",
            "Epoch 7/14\n",
            "200/200 [==============================] - 331s 2s/step - loss: 0.1628 - accuracy: 0.9398 - val_loss: 0.3970 - val_accuracy: 0.8618\n",
            "Epoch 8/14\n",
            "200/200 [==============================] - 296s 1s/step - loss: 0.1387 - accuracy: 0.9503 - val_loss: 0.4331 - val_accuracy: 0.8340\n",
            "Epoch 9/14\n",
            "200/200 [==============================] - 312s 2s/step - loss: 0.1310 - accuracy: 0.9531 - val_loss: 0.4729 - val_accuracy: 0.8472\n",
            "Epoch 10/14\n",
            "200/200 [==============================] - 311s 2s/step - loss: 0.1117 - accuracy: 0.9596 - val_loss: 0.5211 - val_accuracy: 0.8568\n",
            "Epoch 11/14\n",
            "200/200 [==============================] - 310s 2s/step - loss: 0.0867 - accuracy: 0.9692 - val_loss: 0.5774 - val_accuracy: 0.8476\n",
            "Epoch 12/14\n",
            "200/200 [==============================] - 334s 2s/step - loss: 0.0692 - accuracy: 0.9772 - val_loss: 0.5695 - val_accuracy: 0.8408\n",
            "Epoch 13/14\n",
            "200/200 [==============================] - 327s 2s/step - loss: 0.0663 - accuracy: 0.9788 - val_loss: 0.6041 - val_accuracy: 0.8392\n",
            "Epoch 14/14\n",
            "200/200 [==============================] - 315s 2s/step - loss: 0.1146 - accuracy: 0.9569 - val_loss: 0.6236 - val_accuracy: 0.8396\n",
            "best_params [0.006, 160, 340, 14]\n",
            "lr 0.006 n_neurons 160 n_timesteps 340 n_epochs 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/13\n",
            "200/200 [==============================] - 322s 2s/step - loss: 0.5777 - accuracy: 0.6766 - val_loss: 0.3592 - val_accuracy: 0.8494\n",
            "Epoch 2/13\n",
            "200/200 [==============================] - 323s 2s/step - loss: 0.3323 - accuracy: 0.8629 - val_loss: 0.3325 - val_accuracy: 0.8610\n",
            "Epoch 3/13\n",
            "200/200 [==============================] - 316s 2s/step - loss: 0.2947 - accuracy: 0.8841 - val_loss: 0.4502 - val_accuracy: 0.8404\n",
            "Epoch 4/13\n",
            "200/200 [==============================] - 316s 2s/step - loss: 0.2469 - accuracy: 0.9072 - val_loss: 0.3296 - val_accuracy: 0.8626\n",
            "Epoch 5/13\n",
            "200/200 [==============================] - 314s 2s/step - loss: 0.2126 - accuracy: 0.9213 - val_loss: 0.3601 - val_accuracy: 0.8654\n",
            "Epoch 6/13\n",
            "200/200 [==============================] - 314s 2s/step - loss: 0.2008 - accuracy: 0.9268 - val_loss: 0.4035 - val_accuracy: 0.8254\n",
            "Epoch 7/13\n",
            "200/200 [==============================] - 325s 2s/step - loss: 0.1901 - accuracy: 0.9290 - val_loss: 0.3796 - val_accuracy: 0.8622\n",
            "Epoch 8/13\n",
            "200/200 [==============================] - 313s 2s/step - loss: 0.1525 - accuracy: 0.9478 - val_loss: 0.4346 - val_accuracy: 0.8608\n",
            "Epoch 9/13\n",
            "200/200 [==============================] - 318s 2s/step - loss: 0.1391 - accuracy: 0.9506 - val_loss: 0.3989 - val_accuracy: 0.8438\n",
            "Epoch 10/13\n",
            "200/200 [==============================] - 317s 2s/step - loss: 0.1253 - accuracy: 0.9554 - val_loss: 0.4760 - val_accuracy: 0.8554\n",
            "Epoch 11/13\n",
            "200/200 [==============================] - 329s 2s/step - loss: 0.1161 - accuracy: 0.9593 - val_loss: 0.5560 - val_accuracy: 0.8400\n",
            "Epoch 12/13\n",
            "200/200 [==============================] - 324s 2s/step - loss: 0.0905 - accuracy: 0.9689 - val_loss: 0.5486 - val_accuracy: 0.8540\n",
            "Epoch 13/13\n",
            "200/200 [==============================] - 318s 2s/step - loss: 0.0724 - accuracy: 0.9776 - val_loss: 0.6123 - val_accuracy: 0.8480\n",
            "best_params [0.006, 160, 340, 13]\n",
            "lr 0.006 n_neurons 160 n_timesteps 340 n_epochs 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "200/200 [==============================] - 344s 2s/step - loss: 0.5525 - accuracy: 0.7093 - val_loss: 0.3389 - val_accuracy: 0.8622\n",
            "Epoch 2/2\n",
            "200/200 [==============================] - 334s 2s/step - loss: 0.3443 - accuracy: 0.8587 - val_loss: 0.4153 - val_accuracy: 0.8190\n",
            "best_params [0.006, 160, 340, 2]\n",
            "lr 0.006 n_neurons 160 n_timesteps 340 n_epochs 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/14\n",
            "200/200 [==============================] - 313s 2s/step - loss: 0.6132 - accuracy: 0.6539 - val_loss: 0.5508 - val_accuracy: 0.7300\n",
            "Epoch 2/14\n",
            "200/200 [==============================] - 336s 2s/step - loss: 0.4594 - accuracy: 0.7801 - val_loss: 0.4514 - val_accuracy: 0.8132\n",
            "Epoch 3/14\n",
            "200/200 [==============================] - 324s 2s/step - loss: 0.3491 - accuracy: 0.8537 - val_loss: 0.3456 - val_accuracy: 0.8616\n",
            "Epoch 4/14\n",
            "200/200 [==============================] - 326s 2s/step - loss: 0.2684 - accuracy: 0.8950 - val_loss: 0.3397 - val_accuracy: 0.8652\n",
            "Epoch 5/14\n",
            "200/200 [==============================] - 333s 2s/step - loss: 0.2350 - accuracy: 0.9104 - val_loss: 0.3890 - val_accuracy: 0.8582\n",
            "Epoch 6/14\n",
            "200/200 [==============================] - 343s 2s/step - loss: 0.2123 - accuracy: 0.9206 - val_loss: 0.3784 - val_accuracy: 0.8554\n",
            "Epoch 7/14\n",
            "200/200 [==============================] - 339s 2s/step - loss: 0.1814 - accuracy: 0.9340 - val_loss: 0.3927 - val_accuracy: 0.8592\n",
            "Epoch 8/14\n",
            "200/200 [==============================] - 342s 2s/step - loss: 0.1533 - accuracy: 0.9457 - val_loss: 0.4124 - val_accuracy: 0.8540\n",
            "Epoch 9/14\n",
            "200/200 [==============================] - 331s 2s/step - loss: 0.1266 - accuracy: 0.9570 - val_loss: 0.4722 - val_accuracy: 0.8524\n",
            "Epoch 10/14\n",
            "200/200 [==============================] - 322s 2s/step - loss: 0.1417 - accuracy: 0.9495 - val_loss: 0.5176 - val_accuracy: 0.8426\n",
            "Epoch 11/14\n",
            "200/200 [==============================] - 333s 2s/step - loss: 0.1198 - accuracy: 0.9589 - val_loss: 0.4897 - val_accuracy: 0.8442\n",
            "Epoch 12/14\n",
            "200/200 [==============================] - 335s 2s/step - loss: 0.0988 - accuracy: 0.9676 - val_loss: 0.5824 - val_accuracy: 0.8416\n",
            "Epoch 13/14\n",
            "200/200 [==============================] - 326s 2s/step - loss: 0.1219 - accuracy: 0.9546 - val_loss: 0.5403 - val_accuracy: 0.8426\n",
            "Epoch 14/14\n",
            "200/200 [==============================] - 326s 2s/step - loss: 0.0900 - accuracy: 0.9702 - val_loss: 0.5263 - val_accuracy: 0.8278\n",
            "lr 0.006 n_neurons 160 n_timesteps 340 n_epochs 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/13\n",
            "200/200 [==============================] - 348s 2s/step - loss: 0.5351 - accuracy: 0.7151 - val_loss: 0.3872 - val_accuracy: 0.8320\n",
            "Epoch 2/13\n",
            "200/200 [==============================] - 329s 2s/step - loss: 0.3407 - accuracy: 0.8632 - val_loss: 0.3819 - val_accuracy: 0.8450\n",
            "Epoch 3/13\n",
            "200/200 [==============================] - 327s 2s/step - loss: 0.2964 - accuracy: 0.8853 - val_loss: 0.3527 - val_accuracy: 0.8516\n",
            "Epoch 4/13\n",
            "200/200 [==============================] - 326s 2s/step - loss: 0.3297 - accuracy: 0.8644 - val_loss: 0.3763 - val_accuracy: 0.8498\n",
            "Epoch 5/13\n",
            "200/200 [==============================] - 326s 2s/step - loss: 0.2337 - accuracy: 0.9104 - val_loss: 0.3527 - val_accuracy: 0.8600\n",
            "Epoch 6/13\n",
            "200/200 [==============================] - 329s 2s/step - loss: 0.2005 - accuracy: 0.9255 - val_loss: 0.3774 - val_accuracy: 0.8496\n",
            "Epoch 7/13\n",
            "200/200 [==============================] - 328s 2s/step - loss: 0.1695 - accuracy: 0.9377 - val_loss: 0.4160 - val_accuracy: 0.8442\n",
            "Epoch 8/13\n",
            "200/200 [==============================] - 330s 2s/step - loss: 0.1447 - accuracy: 0.9469 - val_loss: 0.4607 - val_accuracy: 0.8528\n",
            "Epoch 9/13\n",
            "200/200 [==============================] - 326s 2s/step - loss: 0.1154 - accuracy: 0.9600 - val_loss: 0.6138 - val_accuracy: 0.8250\n",
            "Epoch 10/13\n",
            "200/200 [==============================] - 337s 2s/step - loss: 0.1134 - accuracy: 0.9592 - val_loss: 0.5525 - val_accuracy: 0.7988\n",
            "Epoch 11/13\n",
            "200/200 [==============================] - 327s 2s/step - loss: 0.0888 - accuracy: 0.9697 - val_loss: 0.6187 - val_accuracy: 0.8452\n",
            "Epoch 12/13\n",
            "200/200 [==============================] - 343s 2s/step - loss: 0.0781 - accuracy: 0.9742 - val_loss: 0.6183 - val_accuracy: 0.8010\n",
            "Epoch 13/13\n",
            "200/200 [==============================] - 352s 2s/step - loss: 0.0780 - accuracy: 0.9724 - val_loss: 0.6046 - val_accuracy: 0.8166\n",
            "lr 0.006 n_neurons 160 n_timesteps 340 n_epochs 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "200/200 [==============================] - 320s 2s/step - loss: 0.5879 - accuracy: 0.6707 - val_loss: 0.4352 - val_accuracy: 0.8212\n",
            "Epoch 2/2\n",
            "200/200 [==============================] - 327s 2s/step - loss: 0.3461 - accuracy: 0.8543 - val_loss: 0.3298 - val_accuracy: 0.8646\n",
            "best_params [0.006, 160, 340, 2]\n",
            "lr 0.006 n_neurons 160 n_timesteps 296 n_epochs 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/14\n",
            "200/200 [==============================] - 328s 2s/step - loss: 0.5363 - accuracy: 0.7074 - val_loss: 0.4684 - val_accuracy: 0.7832\n",
            "Epoch 2/14\n",
            "200/200 [==============================] - 313s 2s/step - loss: 0.3463 - accuracy: 0.8597 - val_loss: 0.3371 - val_accuracy: 0.8612\n",
            "Epoch 3/14\n",
            "200/200 [==============================] - 325s 2s/step - loss: 0.2735 - accuracy: 0.8953 - val_loss: 0.3325 - val_accuracy: 0.8608\n",
            "Epoch 4/14\n",
            "200/200 [==============================] - 335s 2s/step - loss: 0.2423 - accuracy: 0.9104 - val_loss: 0.3546 - val_accuracy: 0.8564\n",
            "Epoch 5/14\n",
            "200/200 [==============================] - 316s 2s/step - loss: 0.2174 - accuracy: 0.9201 - val_loss: 0.3349 - val_accuracy: 0.8640\n",
            "Epoch 6/14\n",
            "200/200 [==============================] - 315s 2s/step - loss: 0.1881 - accuracy: 0.9312 - val_loss: 0.3473 - val_accuracy: 0.8520\n",
            "Epoch 7/14\n",
            "200/200 [==============================] - 310s 2s/step - loss: 0.1558 - accuracy: 0.9441 - val_loss: 0.4456 - val_accuracy: 0.8560\n",
            "Epoch 8/14\n",
            "200/200 [==============================] - 315s 2s/step - loss: 0.1399 - accuracy: 0.9488 - val_loss: 0.4372 - val_accuracy: 0.8532\n",
            "Epoch 9/14\n",
            "200/200 [==============================] - 330s 2s/step - loss: 0.1414 - accuracy: 0.9471 - val_loss: 0.4367 - val_accuracy: 0.8350\n",
            "Epoch 10/14\n",
            "200/200 [==============================] - 326s 2s/step - loss: 0.1030 - accuracy: 0.9635 - val_loss: 0.5596 - val_accuracy: 0.8498\n",
            "Epoch 11/14\n",
            "200/200 [==============================] - 323s 2s/step - loss: 0.0784 - accuracy: 0.9739 - val_loss: 0.6295 - val_accuracy: 0.8480\n",
            "Epoch 12/14\n",
            "200/200 [==============================] - 318s 2s/step - loss: 0.1736 - accuracy: 0.9308 - val_loss: 0.6488 - val_accuracy: 0.6956\n",
            "Epoch 13/14\n",
            "200/200 [==============================] - 329s 2s/step - loss: 0.3408 - accuracy: 0.8472 - val_loss: 0.4335 - val_accuracy: 0.8374\n",
            "Epoch 14/14\n",
            "200/200 [==============================] - 325s 2s/step - loss: 0.1593 - accuracy: 0.9391 - val_loss: 0.5471 - val_accuracy: 0.7836\n",
            "lr 0.006 n_neurons 160 n_timesteps 296 n_epochs 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/13\n",
            "200/200 [==============================] - 295s 1s/step - loss: 0.5558 - accuracy: 0.6866 - val_loss: 0.3475 - val_accuracy: 0.8640\n",
            "Epoch 2/13\n",
            " 28/200 [===>..........................] - ETA: 3:44 - loss: 0.3365 - accuracy: 0.8632"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xnEXZkpxaY-D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}